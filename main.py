import feedparser
import sqlite3
import smtplib
import os
import time
import ssl
from email.mime.text import MIMEText
from email.header import Header
from datetime import datetime
from bs4 import BeautifulSoup
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================

# 1. ç¯å¢ƒå˜é‡
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
SENDER_PASSWORD = os.environ.get("SENDER_PASSWORD")
RECEIVER_EMAIL = os.environ.get("RECEIVER_EMAIL")
LLM_API_KEY = os.environ.get("LLM_API_KEY")
SMTP_SERVER = "smtp.qq.com"
SMTP_PORT = 465

# 2. ğŸ›¡ï¸ ã€ç™½åå•å…œåº•ã€‘ä¸­è‹±æ–‡å…³é”®è¯ (ä¸åŒºåˆ†å¤§å°å†™)
# åŒ…å«è¿™äº›è¯çš„æ ‡é¢˜ï¼Œç›´æ¥å¼ºåˆ¶åˆ¤å®šä¸ºâ€œæ„Ÿå…´è¶£â€ï¼Œä¸ç»è¿‡AI
MUST_HAVE_KEYWORDS = [
    # --- English ---
    "fintech", "financial technology", 
    "machine learning", "deep learning", "neural network",
    "climate risk", "esg", "textual analysis",
    # --- ä¸­æ–‡ ---
    "é‡‘èç§‘æŠ€", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ–‡æœ¬åˆ†æ",
    "æ°”å€™é£é™©", "ESG", "å¤§è¯­è¨€æ¨¡å‹", "é«˜é¢‘äº¤æ˜“", "é‡åŒ–æŠ•èµ„"
]

# 3. ğŸ¤– ã€AI åˆ¤åˆ«æ ‡å‡†ã€‘
USER_INTEREST_DESCRIPTION = """
æˆ‘çš„ç ”ç©¶å…´è¶£éå¸¸å¹¿æ³›ï¼Œè¯·é‡‡å–ã€å®½å®¹ç­–ç•¥ã€‘ï¼Œåªè¦æ–‡ç« ç¬¦åˆä»¥ä¸‹**ä»»æ„ä¸€ä¸ª**æ–¹å‘ï¼Œéƒ½å›ç­” Yesï¼š

1. **é‡‘èç§‘æŠ€ (FinTech)**ï¼šæ¶‰åŠé«˜é¢‘äº¤æ˜“ã€å¸‚åœºå¾®è§‚ç»“æ„ã€æ”¯ä»˜ã€åŒºå—é“¾ã€æ•°å­—è´§å¸åŠè´§å¸ç†è®ºçš„ä»»ä½•è¯é¢˜ã€‚
2. **AIä¸å¤§æ•°æ®**ï¼šé‡‘èä¸­çš„æœºå™¨å­¦ä¹ ã€NLPæ–‡æœ¬åˆ†æã€æƒ…æ„Ÿåˆ†æã€é«˜ç»´æ•°æ®é¢„æµ‹ã€‚
3. **èµ„äº§å®šä»·**ï¼šè‚¡ç¥¨æ”¶ç›Šé¢„æµ‹ã€å› å­æ¨¡å‹ã€é‡åŒ–ç­–ç•¥ã€‚
4. **è®¡é‡**ï¼šå› æœæ¨æ–­æ¨¡å‹ã€è®¡é‡æ¨¡å‹ã€‚

æ³¨æ„ï¼š
- å¯¹äºä¸­æ–‡æ–‡ç« ï¼Œè¯·åŒæ ·åº”ç”¨ä¸Šè¿°æ ‡å‡†ã€‚
- å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä»…æ ¹æ®æ ‡é¢˜åˆ¤æ–­ã€‚
"""

# 4. ğŸ“š æœŸåˆŠ RSS åˆ—è¡¨ (å«è‹±æ–‡é¡¶åˆŠ + ä¸­æ–‡é¡¶åˆŠ)
# æ³¨æ„ï¼šä¸­æ–‡æœŸåˆŠä½¿ç”¨ RSSHub ç”Ÿæˆçš„çŸ¥ç½‘é“¾æ¥
RSS_FEEDS = {
    # === è‹±æ–‡ Top Journals ===
    "Journal of Finance": "https://onlinelibrary.wiley.com/feed/15406261/most-recent",
    "JFE": "https://www.sciencedirect.com/science/journal/0304405X/rss", 
    "RFS": "https://academic.oup.com/rss/site_5378/3126.xml",
    "JFQA": "https://www.cambridge.org/core/rss/product/id/1638F6E6C5C0F911299901594F817173",
    "Management Science": "http://pubsonline.informs.org/action/showFeed?type=etoc&feed=rss&jc=mnsc",
    "Review of Finance": "https://academic.oup.com/rss/site_5409/3133.xml",
    
    # === ä¸­æ–‡ Top Journals (via RSSHub) ===
    # ç»æµç ”ç©¶ (CNKI Code: JJYJ)
    "ç»æµç ”ç©¶": "https://rsshub.app/cnki/journals/JJYJ",
    # ç®¡ç†ä¸–ç•Œ (CNKI Code: GLSJ)
    "ç®¡ç†ä¸–ç•Œ": "https://rsshub.app/cnki/journals/GLSJ",
    # é‡‘èç ”ç©¶ (CNKI Code: JRYJ)
    "é‡‘èç ”ç©¶": "https://rsshub.app/cnki/journals/JRYJ",
    # æ•°é‡ç»æµæŠ€æœ¯ç»æµç ”ç©¶ (CNKI Code: SLJY)
    "æ•°é‡ç»æµæŠ€æœ¯ç»æµç ”ç©¶": "https://rsshub.app/cnki/journals/SLJY",
    # ä¸­å›½å·¥ä¸šç»æµ (CNKI Code: GGYY)
    "ä¸­å›½å·¥ä¸šç»æµ": "https://rsshub.app/cnki/journals/GGYY",
    # ç»æµå­¦(å­£åˆŠ) (CNKI Code: JJXJ)
    "ç»æµå­¦å­£åˆŠ": "https://rsshub.app/cnki/journals/JJXJ"
}

LLM_BASE_URL = "https://api.deepseek.com" 
LLM_MODEL = "deepseek-chat"
DB_FILE = "finance_journals.db"

# ================= æ ¸å¿ƒä»£ç  =================

def get_ai_judgement(title, abstract):
    if not LLM_API_KEY: return False
    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
    
    prompt = f"""
    åˆ¤æ–­è¿™ç¯‡é‡‘èè®ºæ–‡æ˜¯å¦ç¬¦åˆç”¨æˆ·å…´è¶£ã€‚
    
    ã€ç”¨æˆ·å…´è¶£ã€‘ï¼š{USER_INTEREST_DESCRIPTION}
    
    ã€è®ºæ–‡æ ‡é¢˜ã€‘ï¼š{title}
    ã€è®ºæ–‡æ‘˜è¦ã€‘ï¼š{abstract}
    
    è§„åˆ™ï¼š
    1. å®å¯é”™æ€ä¸€åƒï¼Œä¸å¯æ”¾è¿‡ä¸€ä¸ªã€‚åªè¦æœ‰ä¸€ç‚¹ç‚¹ç›¸å…³æ€§ï¼Œå°±å›ç­” "Yes"ã€‚
    2. å¯¹äºä¸­æ–‡æ ‡é¢˜ï¼Œè¯·ç†è§£å…¶è¯­ä¹‰ã€‚
    3. åªå›ç­” "Yes" æˆ– "No"ã€‚
    """
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL, messages=[{"role": "user", "content": prompt}],
            temperature=0.1, max_tokens=5
        )
        return "yes" in response.choices[0].message.content.strip().lower()
    except:
        return False

def clean_html(raw):
    if not raw: return ""
    # ç§»é™¤ RSSHub åŠ å…¥çš„æ— å…³å¹¿å‘Šè¯
    text = BeautifulSoup(raw, "html.parser").get_text(separator=' ')
    return ' '.join(text.split())

def init_db():
    conn = sqlite3.connect(DB_FILE)
    conn.execute('''CREATE TABLE IF NOT EXISTS articles (link TEXT PRIMARY KEY, title TEXT, journal TEXT, published_date TEXT)''')
    conn.close()

def is_new(link):
    conn = sqlite3.connect(DB_FILE)
    exists = conn.execute("SELECT 1 FROM articles WHERE link=?", (link,)).fetchone()
    conn.close()
    return exists is None

def save_article(link, title, journal, pub_date):
    conn = sqlite3.connect(DB_FILE)
    conn.execute("INSERT OR IGNORE INTO articles VALUES (?, ?, ?, ?)", (link, title, journal, pub_date))
    conn.commit()
    conn.close()

def send_email(subject, html):
    if not SENDER_PASSWORD: return False
    msg = MIMEText(html, 'html', 'utf-8')
    msg['From'], msg['To'], msg['Subject'] = Header(SENDER_EMAIL), Header(RECEIVER_EMAIL), Header(subject, 'utf-8')
    try:
        s = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT)
        s.login(SENDER_EMAIL, SENDER_PASSWORD)
        s.sendmail(SENDER_EMAIL, [RECEIVER_EMAIL], msg.as_string())
        s.quit()
        return True
    except Exception as e:
        print(f"Email Error: {e}")
        return False

def run_job():
    print("Job started...")
    monthly_data = {}
    total_new = 0
    interesting_count = 0
    pending_save = []

    # å¿½ç•¥ SSL è¯ä¹¦éªŒè¯ (è§£å†³ RSSHub æœ‰æ—¶è¯ä¹¦æŠ¥é”™çš„é—®é¢˜)
    if hasattr(ssl, '_create_unverified_context'):
        ssl._create_default_https_context = ssl._create_unverified_context

    for journal, url in RSS_FEEDS.items():
        print(f"Checking {journal}...")
        try:
            # å¢åŠ è¶…æ—¶è®¾ç½®ï¼Œå› ä¸º RSSHub æœ‰æ—¶è¾ƒæ…¢
            feed = feedparser.parse(url) 
            
            if not feed.entries:
                print(f"  > Warning: No entries for {journal} (Link might be blocked temporarily)")
                continue

            # ä¸­æ–‡æœŸåˆŠé€šå¸¸ä¸€æ›´å°±æ˜¯ä¸€æœŸ(10-20ç¯‡)ï¼Œæ£€æŸ¥å‰ 20 ç¯‡
            for entry in feed.entries[:20]: 
                title = entry.title
                link = entry.link
                # å¤„ç†æ—¥æœŸæ ¼å¼ï¼Œå¦‚æœè·å–å¤±è´¥é»˜è®¤ç”¨ä»Šå¤©
                date = datetime.now().strftime('%Y-%m-%d')
                if 'published' in entry:
                    try: date = entry.published[:10]
                    except: pass
                
                if is_new(link):
                    # 1. å…³é”®è¯ç™½åå•æ£€æŸ¥ (æ”¯æŒä¸­æ–‡)
                    is_match = False
                    for kw in MUST_HAVE_KEYWORDS:
                        if kw.lower() in title.lower():
                            is_match = True
                            print(f"  >>> [Keyword Match] {title[:30]}...")
                            break
                    
                    summary = clean_html(entry.get('summary') or entry.get('description'))
                    
                    # 2. AI æ£€æŸ¥
                    if not is_match:
                        print(f"  ...asking AI: {title[:30]}...")
                        is_match = get_ai_judgement(title, summary)
                        time.sleep(0.2)

                    if journal not in monthly_data: monthly_data[journal] = []
                    
                    info = {"title": title, "link": link, "date": date, "summary": summary, "is_interesting": is_match}
                    
                    if is_match:
                        monthly_data[journal].insert(0, info)
                        interesting_count += 1
                    else:
                        monthly_data[journal].append(info)
                    
                    pending_save.append(info)
                    total_new += 1
        except Exception as e:
            print(f"Error checking {journal}: {e}")

    if total_new > 0:
        print(f"Found {total_new} articles, {interesting_count} interesting.")
        subject_icon = "ğŸ¤– " if interesting_count > 0 else ""
        
        html = f"""<html><body style="font-family:Arial;">
        <h2>ğŸ“… é¡¶åˆŠæ–‡çŒ®æ›´æ–° (ä¸­è‹±æ–‡æ··æ’)</h2>
        <div style="background:#e8f4fd;padding:10px;margin-bottom:20px;border-radius:5px;">
        <b>ç­›é€‰ç­–ç•¥ï¼š</b>åŒ…å« FinTech/æœºå™¨å­¦ä¹ /æ°”å€™é£é™© ç­‰ä¸­è‹±æ–‡å…³é”®è¯ï¼Œæˆ–ç» AI åˆ¤å®šç¬¦åˆå…´è¶£ã€‚
        </div><hr>"""
        
        for journal, arts in monthly_data.items():
            html += f"<h3 style='background:#f2f2f2;padding:10px;border-left:5px solid #0066cc;'>{journal}</h3><ul>"
            for art in arts:
                if art['is_interesting']:
                    style = "color:#d35400;font-weight:bold;font-size:1.1em;"
                    summ = f"<div style='background:#fff8f0;padding:8px;margin-top:5px;color:#555;font-size:0.9em;'>{art['summary'][:300]}...</div>"
                    icon = "ğŸ’¡"
                else:
                    style, summ, icon = "color:#0066cc;font-weight:bold;", "", ""
                
                html += f"<li style='margin-bottom:15px;'>{icon} <a href='{art['link']}' style='{style}text-decoration:none;'>{art['title']}</a><span style='color:#999;font-size:0.8em;margin-left:10px;'>{art['date']}</span>{summ}</li>"
            html += "</ul>"
        
        html += "</body></html>"

        if send_email(f"{subject_icon}æ–‡çŒ®æ›´æ–°: {interesting_count}ç¯‡ç²¾é€‰ ({total_new}ç¯‡æ–°å¢)", html):
            for art in pending_save: save_article(art['link'], art['title'], art['journal'], art['date'])
            # è‡ªåŠ¨åŒæ­¥æ•°æ®åº“
            os.system('git config --global user.name "Bot" && git config --global user.email "bot@bot.com"')
            os.system('git add finance_journals.db && git commit -m "Update DB" && git pull --rebase origin main && git push')
    else:
        print("No new articles.")

if __name__ == "__main__":
    init_db()
    run_job()
