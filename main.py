import feedparser
import sqlite3
import smtplib
import os
import time
import ssl
import urllib.parse
from email.mime.text import MIMEText
from email.header import Header
from datetime import datetime
from bs4 import BeautifulSoup
from openai import OpenAI

# ================= é…ç½®åŒºåŸŸ =================

# 1. ç¯å¢ƒå˜é‡ (æ— éœ€ä¿®æ”¹ï¼Œå»GitHub Settingså¡«Secrets)
SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
SENDER_PASSWORD = os.environ.get("SENDER_PASSWORD")
RECEIVER_EMAIL = os.environ.get("RECEIVER_EMAIL")
LLM_API_KEY = os.environ.get("LLM_API_KEY")
SMTP_SERVER = "smtp.qq.com"
SMTP_PORT = 465

# 2. ğŸ›¡ï¸ ã€ç™½åå•å…œåº•ã€‘ä¸­è‹±æ–‡å…³é”®è¯ (åŒ…å«å³é€‰ä¸­)
MUST_HAVE_KEYWORDS = [
    # --- English ---
    "fintech", "financial technology", 
    "machine learning", "deep learning", "neural network",
    "climate risk", "esg", "textual analysis",
    # --- ä¸­æ–‡ ---
    "é‡‘èç§‘æŠ€", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ–‡æœ¬åˆ†æ",
    "å¤§è¯­è¨€æ¨¡å‹", "é«˜é¢‘äº¤æ˜“", "é‡åŒ–æŠ•èµ„"
]

# 3. ğŸ¤– ã€AI åˆ¤åˆ«æ ‡å‡†ã€‘
USER_INTEREST_DESCRIPTION = """
æˆ‘çš„ç ”ç©¶å…´è¶£éå¸¸å¹¿æ³›ï¼Œè¯·é‡‡å–ã€å®½å®¹ç­–ç•¥ã€‘ï¼Œåªè¦æ–‡ç« ç¬¦åˆä»¥ä¸‹**ä»»æ„ä¸€ä¸ª**æ–¹å‘ï¼Œéƒ½å›ç­” Yesï¼š

1. **é‡‘èç§‘æŠ€ (FinTech)**ï¼šæ¶‰åŠé«˜é¢‘äº¤æ˜“ã€å¸‚åœºå¾®è§‚ç»“æ„ã€æ”¯ä»˜ã€åŒºå—é“¾ã€æ•°å­—è´§å¸åŠè´§å¸ç†è®ºçš„ä»»ä½•è¯é¢˜ã€‚
2. **AIä¸å¤§æ•°æ®**ï¼šé‡‘èä¸­çš„æœºå™¨å­¦ä¹ ã€NLPæ–‡æœ¬åˆ†æã€æƒ…æ„Ÿåˆ†æã€é«˜ç»´æ•°æ®é¢„æµ‹ã€‚
3. **èµ„äº§å®šä»·**ï¼šè‚¡ç¥¨æ”¶ç›Šé¢„æµ‹ã€å› å­æ¨¡å‹ã€é‡åŒ–ç­–ç•¥ã€‚
4. **è®¡é‡**ï¼šå› æœæ¨æ–­æ¨¡å‹ã€è®¡é‡æ¨¡å‹ã€‚

æ³¨æ„ï¼šå¯¹äºä¸­æ–‡æ–‡ç« åŒç†ã€‚å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä»…æ ¹æ®æ ‡é¢˜åˆ¤æ–­ã€‚
"""

# 4. ğŸ“š æœŸåˆŠ RSS åˆ—è¡¨
RSS_FEEDS = {
    # === è‹±æ–‡ Top Journals ===
    "Journal of Finance": "https://onlinelibrary.wiley.com/feed/15406261/most-recent",
    "JFE": "https://www.sciencedirect.com/science/journal/0304405X/rss", 
    "RFS": "https://academic.oup.com/rss/site_5378/3126.xml",
    "JFQA": "https://www.cambridge.org/core/rss/product/id/1638F6E6C5C0F911299901594F817173",
    "Management Science": "http://pubsonline.informs.org/action/showFeed?type=etoc&feed=rss&jc=mnsc",
    "Review of Finance": "https://academic.oup.com/rss/site_5409/3133.xml",
    
    # === ä¸­æ–‡ Top Journals (via RSSHub) ===
    "ç»æµç ”ç©¶": "https://rsshub.app/cnki/journals/JJYJ",
    "ç®¡ç†ä¸–ç•Œ": "https://rsshub.app/cnki/journals/GLSJ",
    "é‡‘èç ”ç©¶": "https://rsshub.app/cnki/journals/JRYJ",
    "æ•°é‡ç»æµæŠ€æœ¯ç»æµç ”ç©¶": "https://rsshub.app/cnki/journals/SLJY",
    "ä¸­å›½å·¥ä¸šç»æµ": "https://rsshub.app/cnki/journals/GGYY",
    "ç»æµå­¦å­£åˆŠ": "https://rsshub.app/cnki/journals/JJXJ"
}

LLM_BASE_URL = "https://api.deepseek.com" 
LLM_MODEL = "deepseek-chat"
DB_FILE = "finance_journals.db"

# ================= æ ¸å¿ƒä»£ç  =================

def get_zju_vpn_link(original_url):
    """
    å°†æ™®é€šé“¾æ¥è½¬æ¢ä¸ºæµ™å¤§ WebVPN æ ¼å¼
    Ex: https://www.sciencedirect.com/... -> https://www-sciencedirect-com.webvpn.zju.edu.cn/...
    """
    try:
        parsed = urllib.parse.urlparse(original_url)
        # å°†åŸŸåä¸­çš„ . æ›¿æ¢ä¸º -
        domain_trans = parsed.netloc.replace('.', '-')
        # é‡æ–°æ‹¼æ¥
        vpn_url = f"https://{domain_trans}.webvpn.zju.edu.cn{parsed.path}"
        if parsed.query:
            vpn_url += f"?{parsed.query}"
        return vpn_url
    except:
        return "https://webvpn.zju.edu.cn/"

def get_ai_judgement(title, abstract):
    if not LLM_API_KEY: return False
    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
    prompt = f"""
    åˆ¤æ–­è¿™ç¯‡é‡‘èè®ºæ–‡æ˜¯å¦ç¬¦åˆç”¨æˆ·å…´è¶£ã€‚
    ã€ç”¨æˆ·å…´è¶£ã€‘ï¼š{USER_INTEREST_DESCRIPTION}
    ã€è®ºæ–‡æ ‡é¢˜ã€‘ï¼š{title}
    ã€è®ºæ–‡æ‘˜è¦ã€‘ï¼š{abstract}
    è§„åˆ™ï¼š1. å®å¯é”™æ€ä¸€åƒï¼Œä¸å¯æ”¾è¿‡ä¸€ä¸ªã€‚2. åªå›ç­” "Yes" æˆ– "No"ã€‚
    """
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL, messages=[{"role": "user", "content": prompt}],
            temperature=0.1, max_tokens=5
        )
        return "yes" in response.choices[0].message.content.strip().lower()
    except:
        return False

def clean_html(raw):
    if not raw: return ""
    text = BeautifulSoup(raw, "html.parser").get_text(separator=' ')
    return ' '.join(text.split())

def init_db():
    conn = sqlite3.connect(DB_FILE)
    conn.execute('''CREATE TABLE IF NOT EXISTS articles (link TEXT PRIMARY KEY, title TEXT, journal TEXT, published_date TEXT)''')
    conn.close()

def is_new(link):
    conn = sqlite3.connect(DB_FILE)
    exists = conn.execute("SELECT 1 FROM articles WHERE link=?", (link,)).fetchone()
    conn.close()
    return exists is None

def save_article(link, title, journal, pub_date):
    conn = sqlite3.connect(DB_FILE)
    conn.execute("INSERT OR IGNORE INTO articles VALUES (?, ?, ?, ?)", (link, title, journal, pub_date))
    conn.commit()
    conn.close()

def send_email(subject, html):
    if not SENDER_PASSWORD: return False
    msg = MIMEText(html, 'html', 'utf-8')
    msg['From'], msg['To'], msg['Subject'] = Header(SENDER_EMAIL), Header(RECEIVER_EMAIL), Header(subject, 'utf-8')
    try:
        s = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT)
        s.login(SENDER_EMAIL, SENDER_PASSWORD)
        s.sendmail(SENDER_EMAIL, [RECEIVER_EMAIL], msg.as_string())
        s.quit()
        return True
    except Exception as e:
        print(f"Email Error: {e}")
        return False

def run_job():
    print("Job started...")
    monthly_data = {}
    total_new = 0
    interesting_count = 0
    pending_save = []

    # å¿½ç•¥ SSL è¯ä¹¦éªŒè¯
    if hasattr(ssl, '_create_unverified_context'):
        ssl._create_default_https_context = ssl._create_unverified_context

    # ä¼ªè£…æˆ Chrome æµè§ˆå™¨
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    for journal, url in RSS_FEEDS.items():
        print(f"Checking {journal}...")
        try:
            feed = feedparser.parse(url, request_headers=headers)
            
            if not feed.entries:
                print(f"  > Warning: No entries for {journal}")
                continue

            for entry in feed.entries[:20]: 
                title = entry.title
                link = entry.link
                date = datetime.now().strftime('%Y-%m-%d')
                if 'published' in entry:
                    try: date = entry.published[:10]
                    except: pass
                
                if is_new(link):
                    is_match = False
                    # 1. å…³é”®è¯æ£€æŸ¥
                    for kw in MUST_HAVE_KEYWORDS:
                        if kw.lower() in title.lower():
                            is_match = True
                            print(f"  >>> [Keyword Match] {title[:30]}...")
                            break
                    
                    summary = clean_html(entry.get('summary') or entry.get('description'))
                    
                    # 2. AI æ£€æŸ¥
                    if not is_match:
                        print(f"  ...asking AI: {title[:30]}...")
                        is_match = get_ai_judgement(title, summary)
                        time.sleep(0.2) 

                    if journal not in monthly_data: monthly_data[journal] = []
                    
                    info = {
                        "title": title, "link": link, "date": date, 
                        "summary": summary, "is_interesting": is_match, "journal": journal
                    }
                    
                    if is_match:
                        monthly_data[journal].insert(0, info)
                        interesting_count += 1
                    else:
                        monthly_data[journal].append(info)
                    
                    pending_save.append(info)
                    total_new += 1
        except Exception as e:
            print(f"Error checking {journal}: {e}")

    if total_new > 0:
        print(f"Found {total_new} articles, {interesting_count} interesting.")
        subject_icon = "ğŸ¤– " if interesting_count > 0 else ""
        
        html = f"""<html><body style="font-family:Arial;">
        <h2>ğŸ“… é¡¶åˆŠæ–‡çŒ®æ›´æ–° (ä¸­è‹±æ–‡æ··æ’)</h2>
        <div style="background:#e8f4fd;padding:10px;margin-bottom:20px;border-radius:5px;">
        <b>ç­›é€‰ç­–ç•¥ï¼š</b>åŒ…å« FinTech/æœºå™¨å­¦ä¹ /è®¡é‡ ç­‰ä¸­è‹±æ–‡å…³é”®è¯ï¼Œæˆ–ç» AI åˆ¤å®šç¬¦åˆå…´è¶£ã€‚
        </div><hr>"""
        
        for journal, arts in monthly_data.items():
            html += f"<h3 style='background:#f2f2f2;padding:10px;border-left:5px solid #0066cc;'>{journal}</h3><ul>"
            for art in arts:
                # ç”Ÿæˆé“¾æ¥
                search_query = urllib.parse.quote(art['title'])
                google_link = f"https://scholar.google.com/scholar?q={search_query}"
                # ç”Ÿæˆæµ™å¤§ WebVPN é“¾æ¥
                zju_link = get_zju_vpn_link(art['link'])

                if art['is_interesting']:
                    style = "color:#d35400;font-weight:bold;font-size:1.1em;"
                    summ = f"<div style='background:#fff8f0;padding:8px;margin-top:5px;color:#555;font-size:0.9em;'>{art['summary'][:300]}...</div>"
                    icon = "ğŸ’¡"
                else:
                    style, summ, icon = "color:#0066cc;font-weight:bold;", "", ""
                
                html += f"""
                <li style='margin-bottom:15px;'>
                    {icon} 
                    <a href='{art['link']}' style='{style}text-decoration:none;'>{art['title']}</a>
                    <br>
                    <span style='font-size:0.85em; color:#666;'>
                        {art['date']} | 
                        <a href="{zju_link}" style="color:#8e44ad;text-decoration:none;font-weight:bold;">ğŸ« æµ™å¤§WebVPNç›´è¿</a> | 
                        <a href="{google_link}" style="color:#2980b9;text-decoration:none;">ğŸ” Google Scholar</a>
                    </span>
                    {summ}
                </li>"""
            html += "</ul>"
        
        html += "</body></html>"

        if send_email(f"{subject_icon}æ–‡çŒ®æ›´æ–°: {interesting_count}ç¯‡ç²¾é€‰ ({total_new}ç¯‡æ–°å¢)", html):
            for art in pending_save: 
                save_article(art['link'], art['title'], art['journal'], art['date'])
            
            # è‡ªåŠ¨åŒæ­¥æ•°æ®åº“
            os.system('git config --global user.name "Bot" && git config --global user.email "bot@bot.com"')
            os.system('git add finance_journals.db && git commit -m "Update DB" && git pull --rebase origin main && git push')
    else:
        print("No new articles.")

if __name__ == "__main__":
    init_db()
    run_job()
